{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c74ae194-244d-47e7-adf3-8eaa1c4e6341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T01:02:05.422586Z",
     "iopub.status.busy": "2025-09-20T01:02:05.422317Z",
     "iopub.status.idle": "2025-09-20T01:02:26.766905Z",
     "shell.execute_reply": "2025-09-20T01:02:26.766284Z",
     "shell.execute_reply.started": "2025-09-20T01:02:05.422566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 95 nurse datasets\n",
      "Found 95 roster snapshots\n",
      "Dataset shape: (1473200, 17)\n",
      "                         nurse_key               roster_key     dept  day  \\\n",
      "0  raw_data/nurse_data/nurse1.json  historical/roster1.json  General  Tue   \n",
      "1  raw_data/nurse_data/nurse1.json  historical/roster1.json  General  Tue   \n",
      "2  raw_data/nurse_data/nurse1.json  historical/roster1.json  General  Tue   \n",
      "3  raw_data/nurse_data/nurse1.json  historical/roster1.json  General  Tue   \n",
      "4  raw_data/nurse_data/nurse1.json  historical/roster1.json  General  Tue   \n",
      "\n",
      "   shift_type target_nurse candidate_nurse  label  cand_experience  \\\n",
      "0  Full-Night         N003            N001      0                6   \n",
      "1  Full-Night         N003            N002      0                2   \n",
      "2  Full-Night         N003            N003      1                4   \n",
      "3  Full-Night         N003            N004      0                1   \n",
      "4  Full-Night         N003            N005      0                8   \n",
      "\n",
      "  cand_seniority  cand_hours_contract cand_pref                   cand_skills  \\\n",
      "0         Senior                   44   Morning  ICU,General,Pediatrics,ER,OT   \n",
      "1         Junior                   40   Evening  ICU,General,Pediatrics,ER,OT   \n",
      "2            Mid                   40     Night  ICU,General,Pediatrics,ER,OT   \n",
      "3         Junior                   36   Morning  ICU,General,Pediatrics,ER,OT   \n",
      "4         Senior                   44   Morning  ICU,General,Pediatrics,ER,OT   \n",
      "\n",
      "   hours_in_week  would_violate_45  would_violate_8_per_day  has_rest_day  \n",
      "0             44                 1                        1             1  \n",
      "1             40                 1                        1             1  \n",
      "2             40                 1                        1             1  \n",
      "3             36                 0                        1             1  \n",
      "4             44                 1                        1             1  \n",
      "✅ Saved dataset to S3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket_rosters = \"hospital-rosters-history-expendables-us-20250918\"\n",
    "prefix_rosters = \"historical/\"\n",
    "\n",
    "bucket_nurses = \"hospital-roster-data\"\n",
    "prefix_nurses = \"raw_data/nurse_data/\"\n",
    "\n",
    "# --- Helper: shift hours ---\n",
    "def shift_hours(shift_name):\n",
    "    if not shift_name:\n",
    "        return 8\n",
    "    return 4 if \"Half\" in shift_name else 8\n",
    "\n",
    "# --- List nurse data files ---\n",
    "nurse_files = []\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "for page in paginator.paginate(Bucket=bucket_nurses, Prefix=prefix_nurses):\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        if obj[\"Key\"].lower().endswith(\".json\"):\n",
    "            nurse_files.append(obj[\"Key\"])\n",
    "nurse_files.sort()  # ensure order matches rosters\n",
    "\n",
    "print(f\"Found {len(nurse_files)} nurse datasets\")\n",
    "\n",
    "# --- List roster files ---\n",
    "roster_files = []\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "for page in paginator.paginate(Bucket=bucket_rosters, Prefix=prefix_rosters):\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        if obj[\"Key\"].lower().endswith(\".json\"):\n",
    "            roster_files.append(obj[\"Key\"])\n",
    "roster_files.sort()\n",
    "\n",
    "print(f\"Found {len(roster_files)} roster snapshots\")\n",
    "\n",
    "# --- Sanity check ---\n",
    "if len(nurse_files) != len(roster_files):\n",
    "    raise ValueError(\"Mismatch: # of nurse datasets != # of rosters\")\n",
    "\n",
    "# --- Build dataset ---\n",
    "rows = []\n",
    "for nurse_key, roster_key in zip(nurse_files, roster_files):\n",
    "    # load nurse master\n",
    "    obj = s3.get_object(Bucket=bucket_nurses, Key=nurse_key)\n",
    "    nurse_list = json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "    nurse_master = {n[\"nurse_id\"]: n for n in nurse_list}\n",
    "\n",
    "    # load roster\n",
    "    body = s3.get_object(Bucket=bucket_rosters, Key=roster_key)[\"Body\"].read().decode(\"utf-8\")\n",
    "    roster_json = json.loads(body)\n",
    "\n",
    "    # build a weekly lookup: nurse -> their shifts\n",
    "    nurse_shift_map = {}\n",
    "    for dept in roster_json.get(\"departments\", []):\n",
    "        for n in dept.get(\"nurses\", []):\n",
    "            nurse_shift_map.setdefault(n[\"id\"], []).extend(n.get(\"shifts\", []))\n",
    "\n",
    "    # build pairwise rows\n",
    "    for dept in roster_json.get(\"departments\", []):\n",
    "        dept_name = dept.get(\"name\")\n",
    "        for n in dept.get(\"nurses\", []):\n",
    "            assigned_nurse_id = n.get(\"id\")\n",
    "            for s in n.get(\"shifts\", []):\n",
    "                day = s.get(\"day\")\n",
    "                shift_type = s.get(\"shift\")\n",
    "                sh_hours = shift_hours(shift_type)\n",
    "\n",
    "                # candidate pool = all nurses in this nurse dataset\n",
    "                for cand_id, cand_data in nurse_master.items():\n",
    "                    cand_shifts = nurse_shift_map.get(cand_id, [])\n",
    "\n",
    "                    # weekly + daily hours\n",
    "                    total_hours_week = sum(shift_hours(cs[\"shift\"]) for cs in cand_shifts)\n",
    "                    total_hours_day = sum(\n",
    "                        shift_hours(cs[\"shift\"])\n",
    "                        for cs in cand_shifts if cs[\"day\"] == day\n",
    "                    )\n",
    "\n",
    "                    # check rest day: any day off?\n",
    "                    days_worked = {cs[\"day\"] for cs in cand_shifts}\n",
    "                    has_rest_day = 1 if len(days_worked) < 7 else 0\n",
    "\n",
    "                    row = {\n",
    "                        \"nurse_key\": nurse_key,\n",
    "                        \"roster_key\": roster_key,\n",
    "                        \"dept\": dept_name,\n",
    "                        \"day\": day,\n",
    "                        \"shift_type\": shift_type,\n",
    "                        \"target_nurse\": assigned_nurse_id,\n",
    "                        \"candidate_nurse\": cand_id,\n",
    "                        \"label\": 1 if cand_id == assigned_nurse_id else 0,\n",
    "                        # static features\n",
    "                        \"cand_experience\": cand_data.get(\"experience_years\"),\n",
    "                        \"cand_seniority\": cand_data.get(\"seniority_level\"),\n",
    "                        \"cand_hours_contract\": cand_data.get(\"contracted_hours\"),\n",
    "                        \"cand_pref\": \",\".join(cand_data.get(\"preferences\", [])),\n",
    "                        \"cand_skills\": \",\".join(cand_data.get(\"skills\", [])),\n",
    "                        # compliance features\n",
    "                        \"hours_in_week\": total_hours_week,\n",
    "                        \"would_violate_45\": 1 if total_hours_week + sh_hours > 45 else 0,\n",
    "                        \"would_violate_8_per_day\": 1 if total_hours_day + sh_hours > 8 else 0,\n",
    "                        \"has_rest_day\": has_rest_day,\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# Save dataset to S3\n",
    "local_path = \"/tmp/pairwise_weekly_compliance.parquet\"\n",
    "df.to_parquet(local_path, index=False)\n",
    "s3.upload_file(local_path, bucket_nurses, \"training/pairwise_weekly_compliance.parquet\")\n",
    "print(\"✅ Saved dataset to S3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffd7b7f6-de43-4869-ab51-b6d3f2db2749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T01:03:34.589048Z",
     "iopub.status.busy": "2025-09-20T01:03:34.588772Z",
     "iopub.status.idle": "2025-09-20T01:03:46.185944Z",
     "shell.execute_reply": "2025-09-20T01:03:46.185400Z",
     "shell.execute_reply.started": "2025-09-20T01:03:34.589028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train/Test CSV uploaded to s3://hospital-roster-data/xgboost/data/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_nurses = \"hospital-roster-data\"   # same bucket as Cell 1\n",
    "\n",
    "# ---- Load parquet dataset from S3 (output of Cell 1) ----\n",
    "df = pd.read_parquet(\"s3://hospital-roster-data/training/pairwise_weekly_compliance.parquet\")\n",
    "\n",
    "# ---- Feature engineering ----\n",
    "seniority_map = {\"Junior\": 0, \"Mid\": 1, \"Senior\": 2}\n",
    "df[\"cand_seniority_num\"] = df[\"cand_seniority\"].map(seniority_map).fillna(0)\n",
    "df[\"pref_morning\"] = df[\"cand_pref\"].str.contains(\"Morning\", case=False, na=False).astype(int)\n",
    "df[\"pref_evening\"] = df[\"cand_pref\"].str.contains(\"Evening\", case=False, na=False).astype(int)\n",
    "df[\"pref_night\"]   = df[\"cand_pref\"].str.contains(\"Night\",   case=False, na=False).astype(int)\n",
    "\n",
    "for skill in [\"ER\", \"General\", \"ICU\", \"OT\", \"Pediatrics\"]:\n",
    "    df[f\"skill_{skill}\"] = df[\"cand_skills\"].str.contains(skill, case=False, na=False).astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"cand_experience\", \"cand_hours_contract\", \"cand_seniority_num\",\n",
    "    \"pref_morning\", \"pref_evening\", \"pref_night\",\n",
    "    \"skill_ER\", \"skill_General\", \"skill_ICU\", \"skill_OT\", \"skill_Pediatrics\",\n",
    "    \"hours_in_week\", \"would_violate_45\", \"would_violate_8_per_day\", \"has_rest_day\"\n",
    "]\n",
    "\n",
    "df_out = df[feature_cols + [\"label\"]]\n",
    "\n",
    "# ---- Train/test split ----\n",
    "train = df_out.sample(frac=0.8, random_state=42)\n",
    "test = df_out.drop(train.index)\n",
    "\n",
    "# ---- Save locally as CSV ----\n",
    "train_path = \"/tmp/train.csv\"\n",
    "test_path = \"/tmp/test.csv\"\n",
    "train.to_csv(train_path, header=False, index=False)\n",
    "test.to_csv(test_path, header=False, index=False)\n",
    "\n",
    "# ---- Upload to S3 ----\n",
    "prefix = \"xgboost/data\"\n",
    "s3.upload_file(train_path, bucket_nurses, f\"{prefix}/train/train.csv\")\n",
    "s3.upload_file(test_path, bucket_nurses, f\"{prefix}/test/test.csv\")\n",
    "\n",
    "print(f\"✅ Train/Test CSV uploaded to s3://{bucket_nurses}/{prefix}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ab8adf-891a-48b2-9f13-54081784259a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T01:07:22.140371Z",
     "iopub.status.busy": "2025-09-20T01:07:22.140077Z",
     "iopub.status.idle": "2025-09-20T01:12:11.125381Z",
     "shell.execute_reply": "2025-09-20T01:12:11.124869Z",
     "shell.execute_reply.started": "2025-09-20T01:07:22.140350Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2025-09-20-01-07-22-211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-20 01:07:27 Starting - Starting the training job...\n",
      "2025-09-20 01:07:42 Starting - Preparing the instances for training...\n",
      "2025-09-20 01:08:04 Downloading - Downloading input data...\n",
      "2025-09-20 01:08:29 Downloading - Downloading the training image...\n",
      "2025-09-20 01:09:20 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2025-09-20 01:09:27.183 ip-10-0-121-66.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-09-20 01:09:27.205 ip-10-0-121-66.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] Failed to parse hyperparameter eval_metric value ndcg to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] Failed to parse hyperparameter objective value rank:pairwise to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:28:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:28:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:28:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:28:INFO] Train matrix has 1178560 rows and 15 columns\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:28:INFO] Validation matrix has 294640 rows\u001b[0m\n",
      "\u001b[34m[2025-09-20 01:09:28.377 ip-10-0-121-66.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-09-20 01:09:28.378 ip-10-0-121-66.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-09-20 01:09:28.379 ip-10-0-121-66.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-09-20 01:09:28.379 ip-10-0-121-66.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-09-20:01:09:28:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-ndcg:0.88235#011validation-ndcg:0.86526\u001b[0m\n",
      "\u001b[34m[2025-09-20 01:09:29.841 ip-10-0-121-66.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-09-20 01:09:29.844 ip-10-0-121-66.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-ndcg:0.88360#011validation-ndcg:0.86654\u001b[0m\n",
      "\u001b[34m[2]#011train-ndcg:0.88362#011validation-ndcg:0.86670\u001b[0m\n",
      "\u001b[34m[3]#011train-ndcg:0.88431#011validation-ndcg:0.86731\u001b[0m\n",
      "\u001b[34m[4]#011train-ndcg:0.88410#011validation-ndcg:0.86707\u001b[0m\n",
      "\u001b[34m[5]#011train-ndcg:0.88471#011validation-ndcg:0.86750\u001b[0m\n",
      "\u001b[34m[6]#011train-ndcg:0.88459#011validation-ndcg:0.86755\u001b[0m\n",
      "\u001b[34m[7]#011train-ndcg:0.88458#011validation-ndcg:0.86743\u001b[0m\n",
      "\u001b[34m[8]#011train-ndcg:0.88477#011validation-ndcg:0.86757\u001b[0m\n",
      "\u001b[34m[9]#011train-ndcg:0.88471#011validation-ndcg:0.86760\u001b[0m\n",
      "\u001b[34m[10]#011train-ndcg:0.88476#011validation-ndcg:0.86775\u001b[0m\n",
      "\u001b[34m[11]#011train-ndcg:0.88476#011validation-ndcg:0.86773\u001b[0m\n",
      "\u001b[34m[12]#011train-ndcg:0.88483#011validation-ndcg:0.86779\u001b[0m\n",
      "\u001b[34m[13]#011train-ndcg:0.88473#011validation-ndcg:0.86765\u001b[0m\n",
      "\u001b[34m[14]#011train-ndcg:0.88484#011validation-ndcg:0.86782\u001b[0m\n",
      "\u001b[34m[15]#011train-ndcg:0.88487#011validation-ndcg:0.86786\u001b[0m\n",
      "\u001b[34m[16]#011train-ndcg:0.88489#011validation-ndcg:0.86793\u001b[0m\n",
      "\u001b[34m[17]#011train-ndcg:0.88486#011validation-ndcg:0.86791\u001b[0m\n",
      "\u001b[34m[18]#011train-ndcg:0.88499#011validation-ndcg:0.86800\u001b[0m\n",
      "\u001b[34m[19]#011train-ndcg:0.88501#011validation-ndcg:0.86801\u001b[0m\n",
      "\u001b[34m[20]#011train-ndcg:0.88498#011validation-ndcg:0.86800\u001b[0m\n",
      "\u001b[34m[21]#011train-ndcg:0.88501#011validation-ndcg:0.86804\u001b[0m\n",
      "\u001b[34m[22]#011train-ndcg:0.88499#011validation-ndcg:0.86801\u001b[0m\n",
      "\u001b[34m[23]#011train-ndcg:0.88495#011validation-ndcg:0.86799\u001b[0m\n",
      "\u001b[34m[24]#011train-ndcg:0.88496#011validation-ndcg:0.86802\u001b[0m\n",
      "\u001b[34m[25]#011train-ndcg:0.88501#011validation-ndcg:0.86808\u001b[0m\n",
      "\u001b[34m[26]#011train-ndcg:0.88498#011validation-ndcg:0.86807\u001b[0m\n",
      "\u001b[34m[27]#011train-ndcg:0.88499#011validation-ndcg:0.86808\u001b[0m\n",
      "\u001b[34m[28]#011train-ndcg:0.88500#011validation-ndcg:0.86810\u001b[0m\n",
      "\u001b[34m[29]#011train-ndcg:0.88500#011validation-ndcg:0.86810\u001b[0m\n",
      "\u001b[34m[30]#011train-ndcg:0.88503#011validation-ndcg:0.86813\u001b[0m\n",
      "\u001b[34m[31]#011train-ndcg:0.88502#011validation-ndcg:0.86813\u001b[0m\n",
      "\u001b[34m[32]#011train-ndcg:0.88502#011validation-ndcg:0.86813\u001b[0m\n",
      "\u001b[34m[33]#011train-ndcg:0.88501#011validation-ndcg:0.86813\u001b[0m\n",
      "\u001b[34m[34]#011train-ndcg:0.88503#011validation-ndcg:0.86813\u001b[0m\n",
      "\u001b[34m[35]#011train-ndcg:0.88504#011validation-ndcg:0.86813\u001b[0m\n",
      "\u001b[34m[36]#011train-ndcg:0.88503#011validation-ndcg:0.86812\u001b[0m\n",
      "\u001b[34m[37]#011train-ndcg:0.88503#011validation-ndcg:0.86812\u001b[0m\n",
      "\u001b[34m[38]#011train-ndcg:0.88504#011validation-ndcg:0.86812\u001b[0m\n",
      "\u001b[34m[39]#011train-ndcg:0.88502#011validation-ndcg:0.86811\u001b[0m\n",
      "\u001b[34m[40]#011train-ndcg:0.88504#011validation-ndcg:0.86812\u001b[0m\n",
      "\u001b[34m[41]#011train-ndcg:0.88504#011validation-ndcg:0.86813\u001b[0m\n",
      "\u001b[34m[42]#011train-ndcg:0.88505#011validation-ndcg:0.86814\u001b[0m\n",
      "\u001b[34m[43]#011train-ndcg:0.88509#011validation-ndcg:0.86823\u001b[0m\n",
      "\u001b[34m[44]#011train-ndcg:0.88507#011validation-ndcg:0.86845\u001b[0m\n",
      "\u001b[34m[45]#011train-ndcg:0.88507#011validation-ndcg:0.86845\u001b[0m\n",
      "\u001b[34m[46]#011train-ndcg:0.88512#011validation-ndcg:0.86829\u001b[0m\n",
      "\u001b[34m[47]#011train-ndcg:0.88510#011validation-ndcg:0.86823\u001b[0m\n",
      "\u001b[34m[48]#011train-ndcg:0.88513#011validation-ndcg:0.86829\u001b[0m\n",
      "\u001b[34m[49]#011train-ndcg:0.88513#011validation-ndcg:0.86830\u001b[0m\n",
      "\u001b[34m[50]#011train-ndcg:0.88513#011validation-ndcg:0.86829\u001b[0m\n",
      "\u001b[34m[51]#011train-ndcg:0.88516#011validation-ndcg:0.86830\u001b[0m\n",
      "\u001b[34m[52]#011train-ndcg:0.88516#011validation-ndcg:0.86831\u001b[0m\n",
      "\u001b[34m[53]#011train-ndcg:0.88518#011validation-ndcg:0.86832\u001b[0m\n",
      "\u001b[34m[54]#011train-ndcg:0.88518#011validation-ndcg:0.86832\u001b[0m\n",
      "\u001b[34m[55]#011train-ndcg:0.88516#011validation-ndcg:0.86854\u001b[0m\n",
      "\u001b[34m[56]#011train-ndcg:0.88515#011validation-ndcg:0.86853\u001b[0m\n",
      "\u001b[34m[57]#011train-ndcg:0.88516#011validation-ndcg:0.86854\u001b[0m\n",
      "\u001b[34m[58]#011train-ndcg:0.88516#011validation-ndcg:0.86854\u001b[0m\n",
      "\u001b[34m[59]#011train-ndcg:0.88517#011validation-ndcg:0.86855\u001b[0m\n",
      "\u001b[34m[60]#011train-ndcg:0.88516#011validation-ndcg:0.86853\u001b[0m\n",
      "\u001b[34m[61]#011train-ndcg:0.88516#011validation-ndcg:0.86853\u001b[0m\n",
      "\u001b[34m[62]#011train-ndcg:0.88517#011validation-ndcg:0.86855\u001b[0m\n",
      "\u001b[34m[63]#011train-ndcg:0.88517#011validation-ndcg:0.86855\u001b[0m\n",
      "\u001b[34m[64]#011train-ndcg:0.88516#011validation-ndcg:0.86855\u001b[0m\n",
      "\u001b[34m[65]#011train-ndcg:0.88516#011validation-ndcg:0.86855\u001b[0m\n",
      "\u001b[34m[66]#011train-ndcg:0.88516#011validation-ndcg:0.86854\u001b[0m\n",
      "\u001b[34m[67]#011train-ndcg:0.88516#011validation-ndcg:0.86855\u001b[0m\n",
      "\u001b[34m[68]#011train-ndcg:0.88520#011validation-ndcg:0.86829\u001b[0m\n",
      "\u001b[34m[69]#011train-ndcg:0.88520#011validation-ndcg:0.86830\u001b[0m\n",
      "\u001b[34m[70]#011train-ndcg:0.88520#011validation-ndcg:0.86830\u001b[0m\n",
      "\u001b[34m[71]#011train-ndcg:0.88520#011validation-ndcg:0.86830\u001b[0m\n",
      "\u001b[34m[72]#011train-ndcg:0.88520#011validation-ndcg:0.86829\u001b[0m\n",
      "\u001b[34m[73]#011train-ndcg:0.88520#011validation-ndcg:0.86829\u001b[0m\n",
      "\u001b[34m[74]#011train-ndcg:0.88521#011validation-ndcg:0.86829\u001b[0m\n",
      "\u001b[34m[75]#011train-ndcg:0.88518#011validation-ndcg:0.86852\u001b[0m\n",
      "\u001b[34m[76]#011train-ndcg:0.88517#011validation-ndcg:0.86851\u001b[0m\n",
      "\u001b[34m[77]#011train-ndcg:0.88516#011validation-ndcg:0.86849\u001b[0m\n",
      "\u001b[34m[78]#011train-ndcg:0.88518#011validation-ndcg:0.86850\u001b[0m\n",
      "\u001b[34m[79]#011train-ndcg:0.88518#011validation-ndcg:0.86849\u001b[0m\n",
      "\u001b[34m[80]#011train-ndcg:0.88517#011validation-ndcg:0.86848\u001b[0m\n",
      "\u001b[34m[81]#011train-ndcg:0.88518#011validation-ndcg:0.86848\u001b[0m\n",
      "\u001b[34m[82]#011train-ndcg:0.88518#011validation-ndcg:0.86847\u001b[0m\n",
      "\u001b[34m[83]#011train-ndcg:0.88516#011validation-ndcg:0.86846\u001b[0m\n",
      "\u001b[34m[84]#011train-ndcg:0.88518#011validation-ndcg:0.86848\u001b[0m\n",
      "\u001b[34m[85]#011train-ndcg:0.88519#011validation-ndcg:0.86847\u001b[0m\n",
      "\u001b[34m[86]#011train-ndcg:0.88518#011validation-ndcg:0.86849\u001b[0m\n",
      "\u001b[34m[87]#011train-ndcg:0.88518#011validation-ndcg:0.86846\u001b[0m\n",
      "\u001b[34m[88]#011train-ndcg:0.88517#011validation-ndcg:0.86845\u001b[0m\n",
      "\u001b[34m[89]#011train-ndcg:0.88517#011validation-ndcg:0.86845\u001b[0m\n",
      "\u001b[34m[90]#011train-ndcg:0.88517#011validation-ndcg:0.86845\u001b[0m\n",
      "\u001b[34m[91]#011train-ndcg:0.88517#011validation-ndcg:0.86848\u001b[0m\n",
      "\u001b[34m[92]#011train-ndcg:0.88518#011validation-ndcg:0.86848\u001b[0m\n",
      "\u001b[34m[93]#011train-ndcg:0.88516#011validation-ndcg:0.86847\u001b[0m\n",
      "\u001b[34m[94]#011train-ndcg:0.88517#011validation-ndcg:0.86847\u001b[0m\n",
      "\u001b[34m[95]#011train-ndcg:0.88518#011validation-ndcg:0.86847\u001b[0m\n",
      "\u001b[34m[96]#011train-ndcg:0.88516#011validation-ndcg:0.86845\u001b[0m\n",
      "\u001b[34m[97]#011train-ndcg:0.88516#011validation-ndcg:0.86847\u001b[0m\n",
      "\u001b[34m[98]#011train-ndcg:0.88518#011validation-ndcg:0.86846\u001b[0m\n",
      "\u001b[34m[99]#011train-ndcg:0.88516#011validation-ndcg:0.86847\u001b[0m\n",
      "\n",
      "2025-09-20 01:11:38 Uploading - Uploading generated training model\n",
      "2025-09-20 01:11:38 Completed - Training job completed\n",
      "Training seconds: 215\n",
      "Billable seconds: 215\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = \"hospital-roster-data\"\n",
    "prefix = \"xgboost/data\"\n",
    "\n",
    "# ---- XGBoost container image ----\n",
    "xgb_image = image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\")\n",
    "\n",
    "# ---- Estimator definition ----\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",   # upgrade if needed\n",
    "    output_path=f\"s3://{bucket}/xgboost/output\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# ---- Hyperparameters ----\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    objective=\"rank:pairwise\",\n",
    "    eval_metric=\"ndcg\",\n",
    "    num_round=100,\n",
    "    max_depth=6,\n",
    "    eta=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8\n",
    ")\n",
    "\n",
    "# ---- Input channels ----\n",
    "train_input = TrainingInput(f\"s3://{bucket}/{prefix}/train/\", content_type=\"csv\")\n",
    "validation_input = TrainingInput(f\"s3://{bucket}/{prefix}/test/\", content_type=\"csv\")\n",
    "\n",
    "# ---- Launch training job ----\n",
    "xgb_estimator.fit({\"train\": train_input, \"validation\": validation_input})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e06ffd6-deef-4471-bc7d-12f4f4cec1f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T01:12:25.356727Z",
     "iopub.status.busy": "2025-09-20T01:12:25.356457Z",
     "iopub.status.idle": "2025-09-20T01:12:34.109479Z",
     "shell.execute_reply": "2025-09-20T01:12:34.108928Z",
     "shell.execute_reply.started": "2025-09-20T01:12:25.356705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features rebuilt for inference\n",
      "✅ Predictions saved to S3: s3://hospital-roster-data/predictions/nurse_predictions.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Reload dataset (raw features only)\n",
    "df = pd.read_parquet(\"/tmp/pairwise_weekly_compliance.parquet\")\n",
    "\n",
    "# --- Recreate preprocessing (same as in Cell 2) ---\n",
    "seniority_map = {\"Junior\": 0, \"Mid\": 1, \"Senior\": 2}\n",
    "df[\"cand_seniority_num\"] = df[\"cand_seniority\"].map(seniority_map).fillna(0)\n",
    "\n",
    "df[\"pref_morning\"] = df[\"cand_pref\"].str.contains(\"Morning\", case=False, na=False).astype(int)\n",
    "df[\"pref_evening\"] = df[\"cand_pref\"].str.contains(\"Evening\", case=False, na=False).astype(int)\n",
    "df[\"pref_night\"]   = df[\"cand_pref\"].str.contains(\"Night\",   case=False, na=False).astype(int)\n",
    "\n",
    "for skill in [\"ER\", \"General\", \"ICU\", \"OT\", \"Pediatrics\"]:\n",
    "    df[f\"skill_{skill}\"] = df[\"cand_skills\"].str.contains(skill, case=False, na=False).astype(int)\n",
    "\n",
    "# Same feature set used for training\n",
    "feature_cols = [\n",
    "    \"cand_experience\", \"cand_hours_contract\", \"cand_seniority_num\",\n",
    "    \"pref_morning\", \"pref_evening\", \"pref_night\",\n",
    "    \"skill_ER\", \"skill_General\", \"skill_ICU\", \"skill_OT\", \"skill_Pediatrics\",\n",
    "    \"hours_in_week\", \"would_violate_45\", \"would_violate_8_per_day\", \"has_rest_day\"\n",
    "]\n",
    "\n",
    "print(\"✅ Features rebuilt for inference\")\n",
    "\n",
    "# --- Download model from S3 ---\n",
    "bucket = \"hospital-roster-models\"\n",
    "model_key = \"xgboost/nurse_roster_ranker.json\"\n",
    "local_model_path = \"/tmp/nurse_roster_ranker.json\"\n",
    "\n",
    "s3.download_file(bucket, model_key, local_model_path)\n",
    "\n",
    "# --- Load model ---\n",
    "model = xgb.Booster()\n",
    "model.load_model(local_model_path)\n",
    "\n",
    "# --- Run batch predictions ---\n",
    "X = df[feature_cols]\n",
    "dmat = xgb.DMatrix(X)\n",
    "df[\"score\"] = model.predict(dmat)\n",
    "\n",
    "# --- Save predictions back to S3 ---\n",
    "pred_path = \"/tmp/nurse_predictions.parquet\"\n",
    "df.to_parquet(pred_path, index=False)\n",
    "\n",
    "s3.upload_file(pred_path, \"hospital-roster-model\", \"predictions/nurse_predictions.parquet\")\n",
    "print(\"✅ Predictions saved to S3: s3://hospital-roster-model/predictions/nurse_predictions.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8221988-dea5-4009-9173-2c0d217ae367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
